import numpy as np
import pandas as pd
import os
from matplotlib import pyplot as plt
import logging
from typing import Tuple, Optional
from dataclasses import dataclass
import time
import argparse

# Configure logging to track training progress
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('model_training.log'),
        logging.StreamHandler()
    ]
)

@dataclass
class ModelConfig:
    # Network Architecture
    input_size: int = 784        # 28x28 pixels flattened
    hidden_layer_size: int = 512 # Number of neurons in hidden layers
    output_size: int = 10        # 10 possible digits (0-9)
    
    # Adam Optimizer Parameters
    learning_rate: float = 0.001 # Step size for parameter updates
    beta1: float = 0.9          # Exponential decay rate for moment estimates
    beta2: float = 0.999        # Exponential decay rate for squared gradients
    epsilon: float = 1e-8       # Small constant for numerical stability
    
    # Training Process
    iterations: int = 1000      # Maximum number of training epochs
    batch_size: int = 128       # Number of samples per mini-batch
    dev_set_size: float = 0.1   # 10% of data used for validation
    
    # Regularization
    dropout_rate: float = 0.2   # Fraction of neurons to drop during training
    early_stopping_patience: int = 20  # Epochs to wait before early stopping
    learning_rate_decay: float = 0.1   # Factor to reduce learning rate
    min_learning_rate: float = 1e-6     # Minimum learning rate
    decay_patience: int = 5     # Epochs before applying learning rate decay
    
    # Data Augmentation
    use_augmentation: bool = True
    augmentation_factor: int = 3  # Number of augmented samples per original
    noise_std: float = 0.1       # Standard deviation for Gaussian noise

# Create global configuration instance
config = ModelConfig()

class DataPreprocessor:
    """Handles all data loading and preprocessing operations."""
    
    @staticmethod
    def load_and_preprocess_data() -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
        """
        Loads and preprocesses training data, splitting into train and dev sets.
        """
        try:
            logging.info("Loading training data...")
            # Load CSV file into numpy array
            data = pd.read_csv('Data/train.csv')
            data = np.array(data)
            m, n = data.shape  # m = number of samples, n = features + 1 (label)
            
            # Shuffle data with fixed seed for reproducibility
            np.random.seed(42)  # Fixed seed ensures same split every time
            np.random.shuffle(data)
            
            # Calculate development set size based on config
            dev_size = int(config.dev_set_size * m)
            
            # Split into development and training sets
            # Transpose data for easier slicing: shape becomes (features, samples)
            data_dev = data[0:dev_size].T
            Y_dev = data_dev[0]  # First row contains labels
            X_dev = data_dev[1:n]  # Remaining rows are features
            X_dev = X_dev / 255.  # Normalize pixel values to [0,1]

            # Process training data similarly
            data_train = data[dev_size:m].T
            Y_train = data_train[0]
            X_train = data_train[1:n]
            X_train = X_train / 255.
            
            logging.info(f"Train set shape: X={X_train.shape}, Y={Y_train.shape}")
            logging.info(f"Dev set shape: X={X_dev.shape}, Y={Y_dev.shape}")
            
            return X_train, Y_train, X_dev, Y_dev
            
        except FileNotFoundError:
            logging.error("Training data file not found!")
            raise
        except Exception as e:
            logging.error(f"Error loading training data: {str(e)}")
            raise

    @staticmethod
    def load_test_data() -> Tuple[np.ndarray, Optional[np.ndarray]]:
        """
        Loads and preprocesses test data.
        Returns X_test and Y_test (if labels are available)
        """
        try:
            logging.info("Loading test data...")
            data = pd.read_csv('Data/test.csv')
            data = np.array(data)
            
            # Handle test data with or without labels
            if data.shape[1] == 785:  # If labels are present (785 = 784 pixels + 1 label)
                data_test = data.T
                Y_test = data_test[0]  # First row contains labels
                X_test = data_test[1:]  # Remaining rows are features
            else:  # If no labels (784 pixels only)
                data_test = data.T
                Y_test = None
                X_test = data_test
                
            X_test = X_test / 255.  # Normalize pixel values to [0,1]
            logging.info(f"Test set shape: X={X_test.shape}")
            
            return X_test, Y_test
            
        except Exception as e:
            logging.error(f"Error loading test data: {str(e)}")
            raise

    @staticmethod
    def get_batch(X: np.ndarray, Y: np.ndarray, batch_size: int) -> list:
        """
        Creates mini-batches from the data for batch processing.
        
        Args:
            X: Input features matrix (features × samples)
            Y: Labels vector
            batch_size: Size of each batch
        """
        m = X.shape[1]  # Number of samples
        batches = []
        
        # Shuffle data for randomization in training
        permutation = np.random.permutation(m)
        X_shuffled = X[:, permutation]  # Shuffle columns (samples)
        Y_shuffled = Y[permutation]
        
        # Create complete batches
        num_complete_batches = m // batch_size
        for k in range(num_complete_batches):
            start_idx = k * batch_size
            end_idx = (k + 1) * batch_size
            batch_X = X_shuffled[:, start_idx:end_idx]
            batch_Y = Y_shuffled[start_idx:end_idx]
            batches.append((batch_X, batch_Y))
        
        # Handle the last incomplete batch if it exists
        if m % batch_size != 0:
            batch_X = X_shuffled[:, num_complete_batches * batch_size:]
            batch_Y = Y_shuffled[num_complete_batches * batch_size:]
            batches.append((batch_X, batch_Y))
        
        return batches

    @staticmethod
    def augment_data(X: np.ndarray, Y: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """
        Performs data augmentation to increase dataset size and variety.
        Creates 3 versions of each image: original, rotated, and noisy.
        """
        logging.info("Performing data augmentation...")
        X_augmented = []
        Y_augmented = []
        
        for i in range(X.shape[1]):  # Iterate through each sample
            img = X[:, i].reshape(28, 28)  # Reshape to 2D image
            
            # Add original image
            X_augmented.append(X[:, i])
            Y_augmented.append(Y[i])
            
            # Add rotated version (90 degrees)
            rotated = np.rot90(img, k=1, axes=(0, 1))
            X_augmented.append(rotated.reshape(-1))  # Flatten back to 1D
            Y_augmented.append(Y[i])
            
            # Add noisy version (Gaussian noise)
            noisy = img + np.random.normal(0, config.noise_std, img.shape)
            noisy = np.clip(noisy, 0, 1)  # Ensure values stay in [0,1]
            X_augmented.append(noisy.reshape(-1))
            Y_augmented.append(Y[i])
        
        # Convert lists to arrays and transpose X to maintain (features × samples) shape
        X_aug = np.array(X_augmented).T
        Y_aug = np.array(Y_augmented)
        logging.info(f"Augmented data shape: X={X_aug.shape}, Y={Y_aug.shape}")
        
        return X_aug, Y_aug

class NeuralNetwork:
    """
    Implements a three-layer neural network for digit classification.
    Architecture: Input(784) -> Hidden(512) -> Hidden(512) -> Output(10)
    """
    
    @staticmethod
    def initialize_parameters() -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
        """
        Initializes neural network parameters using He initialization for better gradient flow.
        He initialization: weights ~ N(0, sqrt(2/n_in)) where n_in is the number of input neurons
        """
        # First layer: 784 -> 512
        W1 = np.random.randn(config.hidden_layer_size, config.input_size) * np.sqrt(2./config.input_size)
        b1 = np.zeros((config.hidden_layer_size, 1))
        
        # Second layer: 512 -> 512
        W2 = np.random.randn(config.hidden_layer_size, config.hidden_layer_size) * np.sqrt(2./config.hidden_layer_size)
        b2 = np.zeros((config.hidden_layer_size, 1))
        
        # Output layer: 512 -> 10
        W3 = np.random.randn(config.output_size, config.hidden_layer_size) * np.sqrt(2./config.hidden_layer_size)
        b3 = np.zeros((config.output_size, 1))
        
        return W1, b1, W2, b2, W3, b3

    @staticmethod
    def ReLU(Z: np.ndarray) -> np.ndarray:
        """
        Rectified Linear Unit (ReLU) activation function.
        f(x) = max(0, x)
        """
        return np.maximum(0, Z)

    @staticmethod
    def ReLU_derivative(Z: np.ndarray) -> np.ndarray:
        """
        Derivative of ReLU activation function.
        f'(x) = 1 if x > 0, else 0
        """
        return Z > 0

    @staticmethod
    def softmax(Z: np.ndarray) -> np.ndarray:
        """
        Softmax activation function for output layer.
        f(x_i) = exp(x_i) / Σ(exp(x_j))
        
        Numerically stable implementation using max subtraction.
        """
        # Subtract max for numerical stability (prevents overflow)
        Z_shifted = Z - np.max(Z, axis=0, keepdims=True)
        exp_values = np.exp(Z_shifted)
        return exp_values / np.sum(exp_values, axis=0, keepdims=True)

    @staticmethod
    def forward_propagation(W1: np.ndarray, b1: np.ndarray,
                          W2: np.ndarray, b2: np.ndarray,
                          W3: np.ndarray, b3: np.ndarray,
                          X: np.ndarray, training: bool = True) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
        """
        Performs forward propagation through the network.
        
        For each layer:
        1. Linear transformation: Z = WX + b
        2. Activation: A = g(Z) where g is ReLU for hidden layers and softmax for output
        
        Returns intermediate values needed for backpropagation.
        """
        # First hidden layer
        Z1 = np.dot(W1, X) + b1          # Linear: (512, 784) × (784, m) + (512, 1)
        A1 = NeuralNetwork.ReLU(Z1)      # Activation: ReLU(Z1)
        
        # Apply dropout during training
        if training:
            A1 *= np.random.binomial(1, 1-config.dropout_rate, A1.shape) / (1-config.dropout_rate)
        
        # Second hidden layer
        Z2 = np.dot(W2, A1) + b2         # Linear: (512, 512) × (512, m) + (512, 1)
        A2 = NeuralNetwork.ReLU(Z2)      # Activation: ReLU(Z2)
        
        if training:
            A2 *= np.random.binomial(1, 1-config.dropout_rate, A2.shape) / (1-config.dropout_rate)
        
        # Output layer
        Z3 = np.dot(W3, A2) + b3         # Linear: (10, 512) × (512, m) + (10, 1)
        A3 = NeuralNetwork.softmax(Z3)    # Activation: softmax(Z3)
        
        return Z1, A1, Z2, A2, Z3, A3

    @staticmethod
    def backward_propagation(Z1: np.ndarray, A1: np.ndarray,
                           Z2: np.ndarray, A2: np.ndarray,
                           W1: np.ndarray, W2: np.ndarray, W3: np.ndarray,
                           Z3: np.ndarray, A3: np.ndarray,
                           X: np.ndarray, Y: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
        """
        Performs backward propagation to compute gradients.
        
        Uses chain rule to compute gradients with respect to parameters:
        dL/dW = dL/dA × dA/dZ × dZ/dW
        dL/db = dL/dA × dA/dZ × dZ/db = dL/dA × dA/dZ
        """
        m = Y.size  # Number of examples
        
        # Convert labels to one-hot encoding
        one_hot_Y = np.zeros((m, config.output_size))
        one_hot_Y[np.arange(m), Y.astype(int)] = 1
        one_hot_Y = one_hot_Y.T
        
        # Output layer gradients (cross-entropy with softmax)
        dZ3 = A3 - one_hot_Y                    # dL/dZ3 = A3 - Y
        dW3 = (1/m) * np.dot(dZ3, A2.T)         # dL/dW3 = (1/m) × dZ3 × A2^T
        db3 = (1/m) * np.sum(dZ3, axis=1, keepdims=True)  # dL/db3 = (1/m) × Σ(dZ3)
        
        # Second hidden layer gradients
        dZ2 = np.dot(W3.T, dZ3) * NeuralNetwork.ReLU_derivative(Z2)  # dL/dZ2 = W3^T × dZ3 × g'(Z2)
        dW2 = (1/m) * np.dot(dZ2, A1.T)         # dL/dW2 = (1/m) × dZ2 × A1^T
        db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)  # dL/db2 = (1/m) × Σ(dZ2)
        
        # First hidden layer gradients
        dZ1 = np.dot(W2.T, dZ2) * NeuralNetwork.ReLU_derivative(Z1)  # dL/dZ1 = W2^T × dZ2 × g'(Z1)
        dW1 = (1/m) * np.dot(dZ1, X.T)          # dL/dW1 = (1/m) × dZ1 × X^T
        db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)  # dL/db1 = (1/m) × Σ(dZ1)
        
        return dW1, db1, dW2, db2, dW3, db3

    @staticmethod
    def compute_cost(A3: np.ndarray, Y: np.ndarray) -> float:
        """
        Computes the cross-entropy cost with numerical stability.
        
        Cost = -(1/m) × Σ(y_i × log(a_i))
        where y_i is the true label and a_i is the predicted probability
        """
        m = Y.size
        
        # Convert to one-hot encoding
        one_hot_Y = np.zeros((m, config.output_size))
        one_hot_Y[np.arange(m), Y.astype(int)] = 1
        one_hot_Y = one_hot_Y.T
        
        # Add epsilon for numerical stability (prevent log(0))
        epsilon = 1e-15
        cost = -(1/m) * np.sum(one_hot_Y * np.log(A3 + epsilon))
        
        return cost

class ModelTrainer:
    """Handles model training and optimization procedures."""

    @staticmethod
    def update_parameters_with_adam(parameters: dict, gradients: dict, v: dict, s: dict, t: int) -> Tuple[dict, dict, dict]:
        """
        Updates parameters using Adam optimization algorithm.
        
        Adam combines:
        1. Momentum: v = β1×v + (1-β1)×dW
        2. RMSprop: s = β2×s + (1-β2)×dW²
        3. Bias correction: v_corrected = v / (1-β1^t), s_corrected = s / (1-β2^t)
        4. Update: W = W - α×v_corrected/sqrt(s_corrected + ε)
        """
        v_corrected = {}  # Bias-corrected first moment
        s_corrected = {}  # Bias-corrected second moment
        
        # Update for each parameter (W1, b1, W2, b2, W3, b3)
        for param_name in parameters.keys():
            # Moving average of gradients (momentum)
            v[param_name] = config.beta1 * v[param_name] + (1 - config.beta1) * gradients[param_name]
            
            # Moving average of squared gradients (RMSprop)
            s[param_name] = config.beta2 * s[param_name] + (1 - config.beta2) * np.square(gradients[param_name])
            
            # Bias correction
            v_corrected[param_name] = v[param_name] / (1 - np.power(config.beta1, t))
            s_corrected[param_name] = s[param_name] / (1 - np.power(config.beta2, t))
            
            # Parameter update
            parameters[param_name] -= (config.learning_rate * v_corrected[param_name] / 
                                    (np.sqrt(s_corrected[param_name]) + config.epsilon))
        
        return parameters, v, s

    @staticmethod
    def train_model(X_train: np.ndarray, Y_train: np.ndarray, 
                   X_dev: np.ndarray, Y_dev: np.ndarray,
                   initial_parameters: Optional[dict] = None) -> Tuple[dict, list, list]:
        """
        Trains the neural network using mini-batch gradient descent with Adam optimization.
        
        Args:
            X_train: Training features
            Y_train: Training labels
            X_dev: Development features
            Y_dev: Development labels
            initial_parameters: Optional pre-trained parameters for continued training
        """
        logging.info("Starting model training...")
        
        # Initialize or load parameters
        if initial_parameters is None:
            W1, b1, W2, b2, W3, b3 = NeuralNetwork.initialize_parameters()
            parameters = {'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2, 'W3': W3, 'b3': b3}
        else:
            parameters = initial_parameters
        
        # Initialize Adam optimizer parameters
        v = {key: np.zeros_like(value) for key, value in parameters.items()}  # First moment
        s = {key: np.zeros_like(value) for key, value in parameters.items()}  # Second moment
        
        # Initialize tracking variables
        train_costs = []
        dev_costs = []
        train_accuracies = []
        dev_accuracies = []
        best_dev_cost = float('inf')
        patience_counter = 0
        t = 0  # Time step for Adam
        current_lr = config.learning_rate
        
        for epoch in range(config.iterations):
            epoch_cost = 0
            batches = DataPreprocessor.get_batch(X_train, Y_train, config.batch_size)
            
            # Training loop over mini-batches
            for batch_X, batch_Y in batches:
                t += 1  # Increment time step
                
                # Forward propagation with dropout
                Z1, A1, Z2, A2, Z3, A3 = NeuralNetwork.forward_propagation(
                    parameters['W1'], parameters['b1'],
                    parameters['W2'], parameters['b2'],
                    parameters['W3'], parameters['b3'],
                    batch_X, training=True
                )
                
                # Compute batch cost
                batch_cost = NeuralNetwork.compute_cost(A3, batch_Y)
                epoch_cost += batch_cost
                
                # Backward propagation
                gradients = {}
                dW1, db1, dW2, db2, dW3, db3 = NeuralNetwork.backward_propagation(
                    Z1, A1, Z2, A2,
                    parameters['W1'], parameters['W2'], parameters['W3'],
                    Z3, A3, batch_X, batch_Y
                )
                gradients = {'W1': dW1, 'b1': db1, 'W2': dW2, 'b2': db2, 'W3': dW3, 'b3': db3}
                
                # Update parameters using Adam
                parameters, v, s = ModelTrainer.update_parameters_with_adam(
                    parameters, gradients, v, s, t
                )
            
            # Calculate epoch metrics
            epoch_cost /= len(batches)
            train_costs.append(epoch_cost)
            
            # Calculate accuracies
            train_predictions = ModelEvaluator.predict(parameters, X_train)
            train_accuracy = np.mean(train_predictions == Y_train) * 100
            train_accuracies.append(train_accuracy)
            
            dev_predictions = ModelEvaluator.predict(parameters, X_dev)
            dev_accuracy = np.mean(dev_predictions == Y_dev) * 100
            dev_accuracies.append(dev_accuracy)
            
            # Calculate dev cost
            _, _, _, _, _, A3_dev = NeuralNetwork.forward_propagation(
                parameters['W1'], parameters['b1'],
                parameters['W2'], parameters['b2'],
                parameters['W3'], parameters['b3'],
                X_dev, training=False
            )
            dev_cost = NeuralNetwork.compute_cost(A3_dev, Y_dev)
            dev_costs.append(dev_cost)
            
            # Log progress
            logging.info(f"Epoch {epoch + 1:4d}/{config.iterations}: "
                        f"cost = {epoch_cost:.4f}, "
                        f"train_acc = {train_accuracy:.2f}%, "
                        f"dev_acc = {dev_accuracy:.2f}%")
            
            # Early stopping check
            if dev_cost < best_dev_cost:
                best_dev_cost = dev_cost
                patience_counter = 0
                ModelTrainer.save_model_parameters(parameters)
                logging.info(f"New best model saved with dev cost: {dev_cost:.4f}")
            else:
                patience_counter += 1
            
            # Learning rate decay
            if patience_counter >= config.decay_patience and current_lr < config.min_learning_rate:
                current_lr *= config.learning_rate_decay
                logging.info(f"Learning rate decayed to {current_lr:.6f}")
                patience_counter = 0
            
            # Early stopping
            if patience_counter >= config.early_stopping_patience:
                logging.info(f"Early stopping triggered at epoch {epoch + 1}")
                break
        
        # Plot training history
        ModelEvaluator.plot_training_history(train_costs, dev_costs, train_accuracies, dev_accuracies)
        
        return parameters, train_costs, dev_costs

    @staticmethod
    def save_model_parameters(parameters: dict) -> None:
        """Saves model parameters to file."""
        try:
            np.savez('Data/trained_model.npz', **parameters)
            logging.info("Model parameters saved successfully")
        except Exception as e:
            logging.error(f"Error saving model parameters: {str(e)}")

    @staticmethod
    def load_model_parameters() -> dict:
        """Loads model parameters from file."""
        try:
            saved_params = np.load('Data/trained_model.npz')
            parameters = {key: saved_params[key] for key in saved_params.files}
            logging.info("Model parameters loaded successfully")
            return parameters
        except Exception as e:
            logging.error(f"Error loading model parameters: {str(e)}")
            raise

class ModelEvaluator:
    """Handles model evaluation and visualization."""
    
    @staticmethod
    def predict(parameters: dict, X: np.ndarray) -> np.ndarray:
        """
        Generate predictions for input data.
        
        Args:
            parameters: Model weights and biases
            X: Input features (784 × m)
            
        Returns:
            predictions: Class predictions (m,)
        """
        # Forward pass without dropout (training=False)
        _, _, _, _, _, A3 = NeuralNetwork.forward_propagation(
            parameters['W1'], parameters['b1'],
            parameters['W2'], parameters['b2'],
            parameters['W3'], parameters['b3'],
            X, training=False
        )
        # Return class with highest probability for each example
        return np.argmax(A3, axis=0)

    @staticmethod
    def calculate_metrics(predictions: np.ndarray, Y: np.ndarray) -> Tuple[float, dict]:
        """
        Calculate various performance metrics.
        
        Computes:
        - Accuracy
        - Per-class precision
        - Per-class recall
        - Per-class F1 score
        """
        accuracy = np.mean(predictions == Y) * 100
        
        # Calculate per-class metrics
        classes = np.unique(Y)
        metrics = {}
        for c in classes:
            true_class = (Y == c)
            pred_class = (predictions == c)
            
            # Calculate true positives, false positives, false negatives
            true_positives = np.sum(true_class & pred_class)
            false_positives = np.sum(~true_class & pred_class)
            false_negatives = np.sum(true_class & ~pred_class)
            
            # Calculate precision, recall, and F1
            precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0
            recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0
            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
            
            metrics[f"class_{c}"] = {
                "precision": precision,
                "recall": recall,
                "f1": f1
            }
        
        return accuracy, metrics

    @staticmethod
    def visualize_prediction(X: np.ndarray, prediction: int, true_label: Optional[int] = None) -> None:
        """
        Visualize an input image and its prediction.
        
        Args:
            X: Input image (784,)
            prediction: Predicted digit
            true_label: Optional true digit for comparison
        """
        plt.figure(figsize=(4, 4))
        plt.gray()
        plt.imshow(X.reshape(28, 28), interpolation='nearest')
        title = f"Prediction: {prediction}"
        if true_label is not None:
            title += f" (True: {true_label})"
        plt.title(title)
        plt.axis('off')
        plt.show()

    @staticmethod
    def plot_training_history(train_costs: list, dev_costs: list, 
                            train_accuracies: list, dev_accuracies: list) -> None:
        """
        Plot training and development metrics over time.
        Creates a figure with two subplots:
        1. Training and dev costs
        2. Training and dev accuracies
        """
        plt.figure(figsize=(15, 5))
        
        # Plot costs
        plt.subplot(1, 2, 1)
        plt.plot(train_costs, label='Training Cost')
        plt.plot(dev_costs, label='Dev Cost')
        plt.title('Training and Dev Costs')
        plt.xlabel('Epoch')
        plt.ylabel('Cost')
        plt.legend()
        plt.grid(True)
        
        # Plot accuracies
        plt.subplot(1, 2, 2)
        plt.plot(train_accuracies, label='Training Accuracy')
        plt.plot(dev_accuracies, label='Dev Accuracy')
        plt.title('Training and Dev Accuracies')
        plt.xlabel('Epoch')
        plt.ylabel('Accuracy (%)')
        plt.legend()
        plt.grid(True)
        
        plt.tight_layout()
        plt.savefig('training_history.png')
        plt.close()

def parse_arguments():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(description='Train and evaluate MNIST digit classifier.')
    
    parser.add_argument('--continue_training', 
                       action='store_true',
                       help='Continue training from saved model')
    
    parser.add_argument('--evaluate', 
                       action='store_true',
                       help='Only evaluate the model without training')
    
    parser.add_argument('--batch_size',
                       type=int,
                       default=config.batch_size,
                       help='Mini-batch size for training')
    
    parser.add_argument('--learning_rate',
                       type=float,
                       default=config.learning_rate,
                       help='Initial learning rate')
    
    return parser.parse_args()

def main():
    """Main execution function."""
    try:
        # Parse command line arguments
        args = parse_arguments()
        
        # Update config with command line arguments
        config.batch_size = args.batch_size
        config.learning_rate = args.learning_rate
        
        # Load and preprocess data
        logging.info("Starting model pipeline...")
        X_train, Y_train, X_dev, Y_dev = DataPreprocessor.load_and_preprocess_data()
        X_test, Y_test = DataPreprocessor.load_test_data()
        
        # Optional data augmentation
        if config.use_augmentation:
            X_train, Y_train = DataPreprocessor.augment_data(X_train, Y_train)
        
        # Determine whether to train or load model
        if args.evaluate or (args.continue_training and os.path.exists('Data/trained_model.npz')):
            logging.info("Loading existing model...")
            parameters = ModelTrainer.load_model_parameters()
            
            if args.continue_training and not args.evaluate:
                logging.info("Continuing training...")
                parameters, train_costs, dev_costs = ModelTrainer.train_model(
                    X_train, Y_train, X_dev, Y_dev, 
                    initial_parameters=parameters
                )
        else:
            logging.info("Training new model...")
            parameters, train_costs, dev_costs = ModelTrainer.train_model(
                X_train, Y_train, X_dev, Y_dev
            )
        
        # Evaluate model
        logging.info("Evaluating model performance...")
        
        # Training set evaluation
        train_predictions = ModelEvaluator.predict(parameters, X_train)
        train_accuracy, train_metrics = ModelEvaluator.calculate_metrics(train_predictions, Y_train)
        logging.info(f"Training Accuracy: {train_accuracy:.2f}%")
        
        # Development set evaluation
        dev_predictions = ModelEvaluator.predict(parameters, X_dev)
        dev_accuracy, dev_metrics = ModelEvaluator.calculate_metrics(dev_predictions, Y_dev)
        logging.info(f"Dev Accuracy: {dev_accuracy:.2f}%")
        
        # Test set evaluation if labels are available
        if Y_test is not None:
            test_predictions = ModelEvaluator.predict(parameters, X_test)
            test_accuracy, test_metrics = ModelEvaluator.calculate_metrics(test_predictions, Y_test)
            logging.info(f"Test Accuracy: {test_accuracy:.2f}%")
        
        # Visualize some predictions
        logging.info("Visualizing predictions...")
        num_visualizations = min(5, X_test.shape[1])
        for i in range(num_visualizations):
            prediction = ModelEvaluator.predict(parameters, X_test[:, i:i+1])
            true_label = Y_test[i] if Y_test is not None else None
            ModelEvaluator.visualize_prediction(X_test[:, i], prediction[0], true_label)
        
        logging.info("Model pipeline completed successfully")
        
    except Exception as e:
        logging.error(f"Error in main execution: {str(e)}")
        raise

if __name__ == "__main__":
    main()
