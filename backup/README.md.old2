# Deep Neural Network for MNIST Digit Classification

## Project Overview
This project implements a memory-efficient deep neural network for handwritten digit classification using the MNIST dataset. The model features natural learning progression and optimized resource usage, achieving ~98% accuracy.

## Table of Contents
1. [Features](#features)
2. [Mathematical Foundation](#mathematical-foundation)
3. [Architecture Details](#architecture-details)
4. [Memory Optimization](#memory-optimization)
5. [Training Process](#training-process)
6. [Performance Analysis](#performance-analysis)
7. [Usage](#usage)
8. [Visualization](#visualization)

## Features
- Memory-efficient implementation
- Natural learning progression (10% → 98%)
- Dynamic network growth
- Gradient accumulation
- Learning rate scheduling with warmup
- Early stopping with patience
- Comprehensive metrics tracking
- Memory usage monitoring
- Data augmentation
- Batch processing

## Mathematical Foundation

### Neural Network Architecture
- **Input Layer**: 784 neurons (28×28 pixels)
- **Hidden Layers**: Dynamic growth from 128 → 512 neurons
- **Output Layer**: 10 neurons (digits 0-9)

### Key Components

#### 1. Forward Propagation

```python

Z[I] = W[I] * A[I-1] + b[I] # Linear Transformation
A[I] = ReLU(Z[I]) # Activation



```

#### 2. Learning Rate Schedule

```python
if epoch < warmup_epochs:
    lr = initial_lr * (epoch + 1) / warmup_epochs
else:
    lr = initial_lr * decay^((epoch - warmup_epochs) / decay_patience)
```

#### 3. Adam Optimization
```python
v = β1·v + (1-β1)·dW        # Momentum
s = β2·s + (1-β2)·dW²       # RMSprop
v_corrected = v/(1-β1ᵗ)     # Bias correction
W = W - α·v_corrected/√(s_corrected + ε)
```

## Memory Optimization

### 1. Batch Processing
- Dynamic batch size based on available memory
- Gradient accumulation over multiple batches
- Regular garbage collection

### 2. Data Management
- Chunked data loading
- Float32 precision
- Memory monitoring and limits
- Efficient matrix operations

### 3. Network Growth
- Start with smaller network (128 neurons)
- Gradual growth to final size (512 neurons)
- Memory-efficient parameter updates

## Training Process

### Initialization
```python
initial_hidden_size = 128
final_hidden_size = 512
growth_epochs = 20
```

### Learning Progression
1. **Initial Phase (1-5 epochs)**
   - Learning rate warmup
   - ~10-30% accuracy
   - Small network size

2. **Growth Phase (5-50 epochs)**
   - Network size increases
   - Learning rate at peak
   - ~30-90% accuracy

3. **Fine-tuning Phase (50+ epochs)**
   - Full network size
   - Learning rate decay
   - ~90-98% accuracy

### Memory Usage Pattern
- Initial: ~20% system memory
- Peak: ~60% system memory
- Stable: ~40% system memory

## Performance Analysis

### Metrics Tracked
- Training/Dev accuracy
- Per-class precision/recall
- F1 scores
- Memory usage
- Learning rate progression

### Visualization Tools
1. Training History
   - Cost curves
   - Accuracy progression
   - Learning rate schedule

2. Model Analysis
   - Confusion matrix
   - Sample predictions
   - Memory usage graphs

## Usage

### Command Line Arguments
```bash
# Train new model
python Models/FINAL_MODEL.py

# Continue training
python Models/FINAL_MODEL.py --continue_training

# Evaluate only
python Models/FINAL_MODEL.py --evaluate

# Custom parameters
python Models/FINAL_MODEL.py --batch_size 256 --learning_rate 0.001
```

### Configuration
```python
class ModelConfig:
    batch_size: int = 128
    learning_rate: float = 0.001
    max_memory_percent: float = 80.0
    warmup_epochs: int = 5
    growth_epochs: int = 20
```

## Requirements
- Python 3.7+
- NumPy
- Pandas
- Matplotlib
- psutil

## Performance Benchmarks
- Training time: ~2-3 hours
- Memory usage: 2-4GB RAM
- Final accuracy: 98%+
- Initial accuracy: ~10%

## Future Improvements
1. Distributed training support
2. GPU optimization
3. Model quantization
4. Dynamic batch sizing
5. Automated hyperparameter tuning

## References
1. Adam optimizer paper
2. Memory optimization techniques
3. Neural network growth strategies
4. MNIST dataset paper
